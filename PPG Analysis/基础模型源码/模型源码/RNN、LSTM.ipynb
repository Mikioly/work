{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2298717f-a4fb-45c3-ae15-dfa65da7c864",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04e1f8ff-03cc-4c41-b1bc-7daa08e57d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs, T=2,3 #批大小，输入序列长度\n",
    "input_size,hidden_size = 2,3 #输入特征大小，隐藏层特征大小\n",
    "input = torch.randn(bs,T,input_size) #随机初始化一个输入特征序列\n",
    "h_prev = torch.zeros(bs,hidden_size) #初始隐含状态"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "85f2bdd3-388f-48ed-8d77-70f5055fab95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch output\n",
      "tensor([[[-0.6366,  0.6040, -0.2286],\n",
      "         [-0.3126,  0.8897,  0.0635],\n",
      "         [-0.2692,  0.8053,  0.4785]],\n",
      "\n",
      "        [[-0.6001,  0.6333, -0.1659],\n",
      "         [-0.2251,  0.8493,  0.2865],\n",
      "         [-0.3143,  0.7564,  0.5445]]], grad_fn=<TransposeBackward1>)\n",
      "tensor([[[-0.2692,  0.8053,  0.4785],\n",
      "         [-0.3143,  0.7564,  0.5445]]], grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#step1 调用RNN \n",
    "rnn = nn.RNN(input_size,hidden_size,batch_first = True)\n",
    "rnn_output ,state_final = rnn(input,h_prev.unsqueeze(0))\n",
    "print(\"Pytorch output\")\n",
    "print(rnn_output)\n",
    "print(state_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8db3f50e-1ba2-4eee-9f0b-6de64e6e44b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rnn_forward output\n",
      "tensor([[[-0.6366,  0.6040, -0.2286],\n",
      "         [-0.6375,  0.5691, -0.2138],\n",
      "         [-0.5259,  0.4500,  0.0682]],\n",
      "\n",
      "        [[-0.6001,  0.6333, -0.1659],\n",
      "         [-0.5631,  0.4710, -0.0129],\n",
      "         [-0.4894,  0.4850,  0.1203]]], grad_fn=<CopySlices>)\n",
      "tensor([[[-0.5259,  0.4500,  0.0682],\n",
      "         [-0.4894,  0.4850,  0.1203]]], grad_fn=<UnsqueezeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#step2 手写rnn_forward函数\n",
    "def rnn_foward(input ,weight_ih,weight_hh, bias_ih,bias_hh,h_prev ):\n",
    "    bs, T, input_size = input.shape\n",
    "    h_dim = weight_ih.shape[0]\n",
    "    h_out = torch.zeros(bs,T,h_dim) #初始化一个输出矩阵\n",
    "\n",
    "    for t in range(T):\n",
    "        x = input[:,t,:].unsqueeze(2) #获取当前时刻输入,bs * input_size\n",
    "        w_ih_batch = weight_ih.unsqueeze(0).tile(bs,1,1) #bs*h_dim*input_size\n",
    "        w_hh_batch = weight_hh.unsqueeze(0).tile(bs,1,1) #bs*h_dim*h_dim\n",
    "\n",
    "        w_times_x = torch.bmm(w_ih_batch,x).squeeze(-1) #bs*h_dim\n",
    "        w_times_h = torch.bmm(w_hh_batch,h_prev.unsqueeze(2)).squeeze(-1) #bs*h_dim\n",
    "        h_prev = torch.tanh(w_times_x+bias_ih+bias_hh)\n",
    "        h_out[:,t,:] = h_prev\n",
    "    return h_out,h_prev.unsqueeze(0)\n",
    "    \n",
    "    \n",
    "# #验证正确性\n",
    "# for k,v in rnn.named_parameters():\n",
    "#     print(k,v)\n",
    "custom_rnn_output,custom_state_final = rnn_foward(input ,rnn.weight_ih_l0,rnn.weight_hh_l0, rnn.bias_ih_l0,rnn.bias_hh_l0,h_prev)\n",
    "print(\"rnn_forward output\")\n",
    "print(custom_rnn_output)\n",
    "print(custom_state_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "503c5e6d-cd5d-473a-aa3a-725e5c9ad4f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.1692, -0.0495,  0.0935, -0.1825,  0.2068],\n",
      "         [ 0.1206, -0.0570,  0.2523, -0.1263,  0.1092],\n",
      "         [ 0.1638,  0.0158,  0.1109, -0.1812,  0.0871]],\n",
      "\n",
      "        [[ 0.6488,  0.0352,  0.1579,  0.0091,  0.2979],\n",
      "         [ 0.3841,  0.0407,  0.1459, -0.0854,  0.3260],\n",
      "         [ 0.2932,  0.0475,  0.1271, -0.1998,  0.1944]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "tensor([[[ 0.1692, -0.0495,  0.0935, -0.1825,  0.2068],\n",
      "         [ 0.1206, -0.0570,  0.2523, -0.1263,  0.1092],\n",
      "         [ 0.1638,  0.0158,  0.1109, -0.1812,  0.0871]],\n",
      "\n",
      "        [[ 0.6488,  0.0352,  0.1579,  0.0091,  0.2979],\n",
      "         [ 0.3841,  0.0407,  0.1459, -0.0854,  0.3260],\n",
      "         [ 0.2932,  0.0475,  0.1271, -0.1998,  0.1944]]], grad_fn=<CopySlices>)\n"
     ]
    }
   ],
   "source": [
    "#实现LSTM网络\n",
    "bs, T, i_size, h_size = 2,3,4,5\n",
    "input = torch.randn(bs,T,i_size) #输入序列\n",
    "c_0 = torch.randn(bs,h_size) #初始值，不需要训练\n",
    "h_0 = torch.randn(bs,h_size)\n",
    "\n",
    "lstm_layer = nn.LSTM(i_size,h_size,batch_first = True)\n",
    "output, (h_final,c_final) = lstm_layer (input , (h_0.unsqueeze(0), c_0.unsqueeze(0)))\n",
    "# print(output)\n",
    "# for k,v in lstm_layer.named_parameters():\n",
    "#     print(k,v.shape)\n",
    "#自己写LSTM模型\n",
    "print(output)\n",
    "def lstm_forward(input,initial_state,w_ih,w_hh,b_ih,b_hh):\n",
    "    h0,c0 = initial_state\n",
    "    bs,T,i_size = input.shape\n",
    "    h_size = w_ih.shape[0]//4\n",
    "    prev_h = h0\n",
    "    prev_c = c0\n",
    "    batch_w_ih = w_ih.unsqueeze(0).tile(bs,1,1) #bs,4*h_size,i_size\n",
    "    batch_w_hh = w_hh.unsqueeze(0).tile(bs,1,1) #bs,4*h_size,h_size\n",
    "    output_size = h_size\n",
    "    output = torch.zeros(bs,T,output_size) #输出序列\n",
    "\n",
    "    for t in range(T):\n",
    "        x = input[:,t,:] #当前时刻的输入向量 [bs,i_size]\n",
    "        w_times_x = torch.bmm(batch_w_ih,x.unsqueeze(-1)) #bs,4*h_size,1\n",
    "        w_times_x = w_times_x.squeeze(-1) #[bs,4*h_size]\n",
    "\n",
    "        w_times_h_prev = torch.bmm(batch_w_hh,prev_h.unsqueeze(-1)) #bs,4*h_size,1\n",
    "        w_times_h_prev = w_times_h_prev.squeeze(-1) #[bs,4*h_size]\n",
    "        #分别计算输入门 i ，遗忘门 f，cell门 g，输出门 o\n",
    "        i_t = torch.sigmoid(w_times_x[: ,:h_size]+w_times_h_prev[:,:h_size] +b_ih[:h_size]+b_hh[:h_size])\n",
    "        f_t = torch.sigmoid(w_times_x[: ,h_size:2*h_size]+w_times_h_prev[:,h_size:2*h_size] +b_ih[h_size:2*h_size]+b_hh[h_size:2*h_size])\n",
    "        g_t = torch.tanh(w_times_x[: ,2*h_size:3*h_size]+w_times_h_prev[:,2*h_size:3*h_size] +b_ih[2*h_size:3*h_size]+b_hh[2*h_size:3*h_size])\n",
    "        o_t = torch.sigmoid(w_times_x[: ,3*h_size:4*h_size]+w_times_h_prev[:,3*h_size:4*h_size] +b_ih[3*h_size:4*h_size]+b_hh[3*h_size:4*h_size])\n",
    "        prev_c = f_t*prev_c + i_t*g_t\n",
    "        prev_h = o_t * torch.tanh(prev_c)\n",
    "\n",
    "        output[:,t,:] = prev_h\n",
    "    return output,(prev_h,prev_c)\n",
    "\n",
    "output_custom, (h_final_custom,c_final_custom) =lstm_forward(input,(h_0,c_0),lstm_layer.weight_ih_l0,lstm_layer.weight_hh_l0,lstm_layer.bias_ih_l0,lstm_layer.bias_hh_l0)\n",
    "print(output_custom)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae74f94b-be4e-4c68-b7e9-0d92479ba2b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
